import logging
from dataclasses import dataclass

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from py_fade.dataset.completion import PromptCompletion
    from py_fade.dataset.completion_logprobs import PromptCompletionLogprobs

@dataclass
class LLMPTokenLogProbs:
    """
    Token logprobs for a single token position.
    """
    token: str
    logprob: float
    top_logprobs: list[tuple[str, float]]|None = None # top N token logprobs if available

    def find_logprob(self, search_token: str) -> float|None:
        """
        Find logprob for given token in top_logprobs if available.
        Returns logprob if found, None otherwise.
        """
        if self.top_logprobs:
            for tok, lp in self.top_logprobs:
                if tok == search_token:
                    return lp
        return None
    
    @classmethod
    def from_list_of_tuples(cls, data: list[tuple[str, float]]) -> list["LLMPTokenLogProbs"]:
        """
        Create list of LLMPTokenLogProbs from list of (token, logprob) tuples.
        """
        return [cls(token=t[0], logprob=t[1]) for t in data]
    
    @classmethod
    def to_list_of_tuples(cls, data: list["LLMPTokenLogProbs"]) -> list[tuple[str, float]]:
        """
        Convert list of LLMPTokenLogProbs to list of (token, logprob) tuples.
        """
        return [(t.token, t.logprob) for t in data]

@dataclass
class LLMResponse:
    """
    Convenience wrapper across providers to hold response text and metadata.
    """
    model_id: str
    full_history: list[dict] # system, user, and previous turn messages used to generate, but not including prefill
    full_response_text: str # full text including prefill and beam_token if any
    response_text: str # text generated by model, excluding prefill and beam_token if any
    temperature: float
    top_k: int
    context_length: int
    max_tokens: int
    min_logprob: float|None = None # min(logprobs), minimal logprob of any token in response
    prefill: str|None = None # part of completion that wasn't generated, but artificially inserted manually or during beam expansion
    beam_token: str|None = None # token at which beam tree was forked, if any
    logprobs: list[LLMPTokenLogProbs]|None = None # per-token logprobs if available
    is_truncated: bool|None = None # whether generation stopped due to max_tokens limit
    is_full_response_logprobs: bool|None = None # whether logprobs cover full_response_text (True) or just part (False). If not computed, None.
    def __post_init__(self):
        #print(f"LLMResponse created with logprobs {len(self.logprobs) if self.logprobs else 0}")
        if self.logprobs and len(self.logprobs) > 0:
            logprob_values = [lp.logprob for lp in self.logprobs if lp.logprob is not None]
            #print(f"Logprob values: {logprob_values}")
            if logprob_values:
                self.min_logprob = min(logprob_values)
                #print(f"Computed min_logprob: {self.min_logprob}")

    def check_full_response_logprobs(self):
        """
        Check if logprobs cover the full response text.
        Go through logprobs and match tokens to full_response_text.
        True if all tokens match and cover full text, False otherwise.
        """
        if not self.is_full_response_logprobs is None:
            return self.is_full_response_logprobs
        if not self.logprobs:
            print("No logprobs available to match full response.")
            return False
        result = None
        resp_pos = 0
        for lp in self.logprobs:
            if lp.logprob is None:
                print("Logprob is None, cannot match full response logprobs.")
                result = False
                break
            # Check if token matches the response text at current position
            if self.full_response_text[resp_pos:resp_pos+len(lp.token)] != lp.token:
                logging.getLogger("LLMResponse").warning(f"Logprob token '{lp.token}' does not match response text at position {resp_pos}, '{self.full_response_text[resp_pos:resp_pos+len(lp.token)]}'.")
                print(f"Logprob token '{lp.token}' does not match response text at position {resp_pos}, '{self.full_response_text[resp_pos:resp_pos+len(lp.token)]}'.")
                result = False
                break
            resp_pos += len(lp.token)
        if result is None:
            result = resp_pos == len(self.full_response_text)
        self.is_full_response_logprobs = result
        return result
    
    @classmethod
    def from_completion_and_logprobs(cls, completion: "PromptCompletion", logprobs: "PromptCompletionLogprobs") -> "LLMResponse":
        """
        Create LLMResponse from PromptCompletion and associated PromptCompletionLogprobs.
        """
        if not logprobs.logprobs or len(logprobs.logprobs) == 0:
            raise ValueError("PromptCompletionLogprobs does not contain logprobs.")
        converted_logprobs = []
        for lp in logprobs.logprobs:
            converted_logprobs.append(LLMPTokenLogProbs(token=lp['token'], logprob=lp['logprob'], top_logprobs=lp.get('top_logprobs')))

        return cls(
            model_id=logprobs.logprobs_model_id,
            full_history=[{"role": "user", "content": completion.prompt_revision.prompt_text}],
            full_response_text=completion.completion_text,
            response_text=completion.completion_text,
            temperature=completion.temperature,
            top_k=completion.top_k,
            prefill=completion.prefill,
            beam_token=completion.beam_token,
            context_length=completion.context_length,
            max_tokens=completion.max_tokens,
            logprobs=converted_logprobs,
            is_truncated=completion.is_truncated,
        )