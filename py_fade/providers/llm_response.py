"""LLM response data structures and processing utilities."""

from dataclasses import dataclass
from typing import TYPE_CHECKING

from py_fade.data_formats.base_data_classes import CommonCompletionProtocol, CommonCompletionLogprobs, CommonConversation

if TYPE_CHECKING:
    from py_fade.dataset.completion import PromptCompletion
    from py_fade.dataset.completion_logprobs import PromptCompletionLogprobs

# @dataclass(slots=True)
# class LLMResponseLogprobs(CommonCompletionLogprobs):
#     """
#     Logprobs associated with an LLMResponse for a specific model_id.
#     """
#     logprobs_model_id: str  # Model ID for which these logprobs were generated.
#     logprobs: list[SinglePositionTokenLogprobs]  # List of token information and logprobs for each position.
#     min_logprob: float | None = None  # min(logprobs), minimal logprob of any token in response
#     avg_logprob: float | None = None  # average(logprobs), average logprob of all tokens in response

#     @classmethod
#     def from_sequence(
#         cls, logprobs_model_id: str, *sequence: "list[SinglePositionTokenLogprobs] | "
#         "SinglePositionTokenLogprobs | CommonCompletionLogprobsProtocol | "
#         "LLMResponseLogprobs | None"
#     ) -> "LLMResponseLogprobs":
#         """
#         Create LLMResponseLogprobs from a sequence of logprobs, supporting mixing different input types.
#         """
#         logprobs = []
#         for item in sequence:
#             if isinstance(item, cls):
#                 logprobs.extend(item.logprobs)
#             elif isinstance(item, list):
#                 logprobs.extend(item)
#             elif isinstance(item, SinglePositionTokenLogprobs):
#                 logprobs.append(item)
#             else:
#                 continue
#         return cls(logprobs_model_id=logprobs_model_id, logprobs=logprobs)

#     @property
#     def first_token_top_logprobs(self) -> list[tuple[str, float]]:
#         """
#         Return top logprobs for the first token of current completion, if available.
#         """
#         if self.logprobs and len(self.logprobs) > 0:
#             first_token_logprobs = self.logprobs[0]
#             if first_token_logprobs.top_logprobs:
#                 return first_token_logprobs.top_logprobs
#         return []

#     def __bool__(self):
#         """
#         True if logprobs are available, False otherwise.
#         """
#         return len(self.logprobs) > 0

#     def __post_init__(self):
#         if self.logprobs:
#             logprob_values = [lp.logprob for lp in self.logprobs if lp.logprob is not None]
#             if logprob_values:
#                 self.min_logprob = min(logprob_values)
#                 self.avg_logprob = sum(logprob_values) / len(logprob_values)

#     def __iter__(self):
#         return iter(self.logprobs)

#     def __len__(self):
#         return len(self.logprobs)

#     def __getitem__(self, key) -> "LLMResponseLogprobs":
#         if isinstance(key, slice):
#             logprobs = self.logprobs.__getitem__(key)
#         else:
#             logprobs = [self.logprobs[key]]  # Single item, wrap in list
#         return LLMResponseLogprobs(
#             logprobs_model_id=self.logprobs_model_id,
#             logprobs=logprobs,
#         )
LLMResponseLogprobs = CommonCompletionLogprobs  # Alias for compatibility, as no extra fields are needed


@dataclass(slots=True)
class LLMResponse(CommonCompletionProtocol):
    """
    Convenience wrapper across providers to hold response text and metadata.
    """

    model_id: str
    prompt_conversation: CommonConversation  # converation leading to this response (excluding prefill and response itself)
    completion_text: str  # full text including prefill and beam_token if any
    generated_part_text: str  # text generated by model, excluding prefill and beam_token if any
    temperature: float
    top_k: int
    context_length: int
    max_tokens: int
    parent_completion_id: int | None = None  # ID of parent completion if this is a derivative, else None
    prefill: str | None = None  # part of completion that wasn't generated, but artificially
    # inserted manually or during beam expansion
    beam_token: str | None = None  # token at which beam tree was forked, if any
    logprobs: CommonCompletionLogprobs | None = None  # per-token logprobs if available
    is_truncated: bool | None = None  # whether generation stopped due to max_tokens limit
    is_full_response_logprobs: bool | None = None  # whether logprobs cover full_response_text (True) or just part (False).
    is_archived: bool = False  # whether this completion is archived and hidden by default
    is_manual: bool = False  # whether this completion was manually edited

    @classmethod
    def from_completion_and_logprobs(cls, completion: "PromptCompletion", logprobs: "PromptCompletionLogprobs") -> "LLMResponse":
        """
        Create LLMResponse from PromptCompletion and associated PromptCompletionLogprobs.
        """
        if not logprobs.sampled_logprobs:
            raise ValueError("Missing sampled_logprobs in provided PromptCompletionLogprobs.")
        llm_response_logprobs = logprobs

        return cls(
            model_id=logprobs.logprobs_model_id,
            prompt_conversation=completion.prompt_conversation,
            completion_text=completion.completion_text,
            generated_part_text=completion.completion_text,
            temperature=completion.temperature,
            top_k=completion.top_k,
            prefill=completion.prefill,
            beam_token=completion.beam_token,
            context_length=completion.context_length,
            max_tokens=completion.max_tokens,
            logprobs=llm_response_logprobs,
            is_truncated=completion.is_truncated,
        )

    def get_logprobs_for_model_id(self, model_id: str) -> CommonCompletionLogprobs | None:
        """
        Get logprobs for the given model ID, if available.

        As LLMResponse is always per single model_id, just either return only available logprobs or None.
        Returns `CommonCompletionLogprobs` compatible result or None if logprobs for the target model_id are not available.
        """
        if self.logprobs and self.model_id == model_id:
            return self.logprobs
        return None
