{
  "results": {
    "mmlu_pro_plus": {
      "exact_match,custom-extract": 0.3657142857142857,
      "exact_match_stderr,custom-extract": 0.012442819121119237,
      "alias": "mmlu_pro_plus"
    },
    "mmlu_pro_plus_biology": {
      "alias": " - biology",
      "exact_match,custom-extract": 0.52,
      "exact_match_stderr,custom-extract": 0.05021167315686779
    },
    "mmlu_pro_plus_business": {
      "alias": " - business",
      "exact_match,custom-extract": 0.32,
      "exact_match_stderr,custom-extract": 0.046882617226215034
    },
    "mmlu_pro_plus_chemistry": {
      "alias": " - chemistry",
      "exact_match,custom-extract": 0.13,
      "exact_match_stderr,custom-extract": 0.03379976689896308
    },
    "mmlu_pro_plus_computer_science": {
      "alias": " - computer_science",
      "exact_match,custom-extract": 0.35,
      "exact_match_stderr,custom-extract": 0.0479372485441102
    },
    "mmlu_pro_plus_economics": {
      "alias": " - economics",
      "exact_match,custom-extract": 0.51,
      "exact_match_stderr,custom-extract": 0.05024183937956912
    },
    "mmlu_pro_plus_engineering": {
      "alias": " - engineering",
      "exact_match,custom-extract": 0.17,
      "exact_match_stderr,custom-extract": 0.03775251680686371
    },
    "mmlu_pro_plus_health": {
      "alias": " - health",
      "exact_match,custom-extract": 0.46,
      "exact_match_stderr,custom-extract": 0.05009082659620332
    },
    "mmlu_pro_plus_history": {
      "alias": " - history",
      "exact_match,custom-extract": 0.5,
      "exact_match_stderr,custom-extract": 0.050251890762960605
    },
    "mmlu_pro_plus_law": {
      "alias": " - law",
      "exact_match,custom-extract": 0.34,
      "exact_match_stderr,custom-extract": 0.04760952285695235
    },
    "mmlu_pro_plus_math": {
      "alias": " - math",
      "exact_match,custom-extract": 0.22,
      "exact_match_stderr,custom-extract": 0.0416333199893227
    },
    "mmlu_pro_plus_other": {
      "alias": " - other",
      "exact_match,custom-extract": 0.33,
      "exact_match_stderr,custom-extract": 0.04725815626252604
    },
    "mmlu_pro_plus_philosophy": {
      "alias": " - philosophy",
      "exact_match,custom-extract": 0.44,
      "exact_match_stderr,custom-extract": 0.04988876515698589
    },
    "mmlu_pro_plus_physics": {
      "alias": " - physics",
      "exact_match,custom-extract": 0.27,
      "exact_match_stderr,custom-extract": 0.044619604333847394
    },
    "mmlu_pro_plus_psychology": {
      "alias": " - psychology",
      "exact_match,custom-extract": 0.56,
      "exact_match_stderr,custom-extract": 0.04988876515698589
    }
  },
  "groups": {
    "mmlu_pro_plus": {
      "exact_match,custom-extract": 0.3657142857142857,
      "exact_match_stderr,custom-extract": 0.012442819121119237,
      "alias": "mmlu_pro_plus"
    }
  },
  "group_subtasks": {
    "mmlu_pro_plus": [
      "mmlu_pro_plus_biology",
      "mmlu_pro_plus_business",
      "mmlu_pro_plus_chemistry",
      "mmlu_pro_plus_computer_science",
      "mmlu_pro_plus_economics",
      "mmlu_pro_plus_engineering",
      "mmlu_pro_plus_health",
      "mmlu_pro_plus_history",
      "mmlu_pro_plus_law",
      "mmlu_pro_plus_math",
      "mmlu_pro_plus_other",
      "mmlu_pro_plus_philosophy",
      "mmlu_pro_plus_physics",
      "mmlu_pro_plus_psychology"
    ]
  },
  "configs": {
    "mmlu_pro_plus_biology": {
      "task": "mmlu_pro_plus_biology",
      "task_alias": "biology",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa84163d2d0>, subject='biology')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7a30>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about biology. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163e290>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_business": {
      "task": "mmlu_pro_plus_business",
      "task_alias": "business",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa84163d240>, subject='business')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7520>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about business. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163dea0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_chemistry": {
      "task": "mmlu_pro_plus_chemistry",
      "task_alias": "chemistry",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa84163d000>, subject='chemistry')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163c3a0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about chemistry. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163c310>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_computer_science": {
      "task": "mmlu_pro_plus_computer_science",
      "task_alias": "computer_science",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa84163c8b0>, subject='computer science')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c5f30>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about computer science. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c79a0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_economics": {
      "task": "mmlu_pro_plus_economics",
      "task_alias": "economics",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa84163c040>, subject='economics')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163cf70>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about economics. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa84163ce50>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_engineering": {
      "task": "mmlu_pro_plus_engineering",
      "task_alias": "engineering",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c6e60>, subject='engineering')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c69e0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about engineering. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7640>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_health": {
      "task": "mmlu_pro_plus_health",
      "task_alias": "health",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c7130>, subject='health')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c6320>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about health. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7370>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_history": {
      "task": "mmlu_pro_plus_history",
      "task_alias": "history",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c7f40>, subject='history')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7c70>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about history. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7b50>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_law": {
      "task": "mmlu_pro_plus_law",
      "task_alias": "law",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c6b00>, subject='law')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c64d0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about law. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c6560>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_math": {
      "task": "mmlu_pro_plus_math",
      "task_alias": "math",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c6830>, subject='math')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa841cc1bd0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about math. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c7880>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_other": {
      "task": "mmlu_pro_plus_other",
      "task_alias": "other",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c5900>, subject='other')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c5c60>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about other. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c5e10>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_philosophy": {
      "task": "mmlu_pro_plus_philosophy",
      "task_alias": "philosophy",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c6290>, subject='philosophy')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa841c6cdc0>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about philosophy. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa841cc1a20>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_physics": {
      "task": "mmlu_pro_plus_physics",
      "task_alias": "physics",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa8413c5ea0>, subject='physics')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c5a20>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about physics. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa8413c5990>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    },
    "mmlu_pro_plus_psychology": {
      "task": "mmlu_pro_plus_psychology",
      "task_alias": "psychology",
      "dataset_path": "saeidasgari/mmlu-pro-plus",
      "test_split": "test",
      "fewshot_split": "validation",
      "process_docs": "functools.partial(<function process_docs at 0x7fa841c6cd30>, subject='psychology')",
      "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa841c6ce50>, including_answer=False)",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "description": "The following are multiple choice questions (with answers) about psychology. Think step by step and then finish your answer with \"the answer is (X)\" where X is the correct letter choice.\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "doc_to_text": "functools.partial(<function format_cot_example at 0x7fa841c6c9d0>, including_answer=True)",
        "doc_to_target": ""
      },
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>",
          "Q:",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "custom-extract",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "answer is \\(?([ABCDEFGHIJKL])\\)?"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "/workspace/gemma3_12b_u2",
        "dtype": "auto",
        "gpu_memory_utilization": 0.9,
        "max_model_len": 4096
      }
    }
  },
  "versions": {
    "mmlu_pro_plus": 1.0,
    "mmlu_pro_plus_biology": 1.0,
    "mmlu_pro_plus_business": 1.0,
    "mmlu_pro_plus_chemistry": 1.0,
    "mmlu_pro_plus_computer_science": 1.0,
    "mmlu_pro_plus_economics": 1.0,
    "mmlu_pro_plus_engineering": 1.0,
    "mmlu_pro_plus_health": 1.0,
    "mmlu_pro_plus_history": 1.0,
    "mmlu_pro_plus_law": 1.0,
    "mmlu_pro_plus_math": 1.0,
    "mmlu_pro_plus_other": 1.0,
    "mmlu_pro_plus_philosophy": 1.0,
    "mmlu_pro_plus_physics": 1.0,
    "mmlu_pro_plus_psychology": 1.0
  },
  "n-shot": {
    "mmlu_pro_plus_biology": 5,
    "mmlu_pro_plus_business": 5,
    "mmlu_pro_plus_chemistry": 5,
    "mmlu_pro_plus_computer_science": 5,
    "mmlu_pro_plus_economics": 5,
    "mmlu_pro_plus_engineering": 5,
    "mmlu_pro_plus_health": 5,
    "mmlu_pro_plus_history": 5,
    "mmlu_pro_plus_law": 5,
    "mmlu_pro_plus_math": 5,
    "mmlu_pro_plus_other": 5,
    "mmlu_pro_plus_philosophy": 5,
    "mmlu_pro_plus_physics": 5,
    "mmlu_pro_plus_psychology": 5
  },
  "higher_is_better": {
    "mmlu_pro_plus": {
      "exact_match": true
    },
    "mmlu_pro_plus_biology": {
      "exact_match": true
    },
    "mmlu_pro_plus_business": {
      "exact_match": true
    },
    "mmlu_pro_plus_chemistry": {
      "exact_match": true
    },
    "mmlu_pro_plus_computer_science": {
      "exact_match": true
    },
    "mmlu_pro_plus_economics": {
      "exact_match": true
    },
    "mmlu_pro_plus_engineering": {
      "exact_match": true
    },
    "mmlu_pro_plus_health": {
      "exact_match": true
    },
    "mmlu_pro_plus_history": {
      "exact_match": true
    },
    "mmlu_pro_plus_law": {
      "exact_match": true
    },
    "mmlu_pro_plus_math": {
      "exact_match": true
    },
    "mmlu_pro_plus_other": {
      "exact_match": true
    },
    "mmlu_pro_plus_philosophy": {
      "exact_match": true
    },
    "mmlu_pro_plus_physics": {
      "exact_match": true
    },
    "mmlu_pro_plus_psychology": {
      "exact_match": true
    }
  },
  "n-samples": {
    "mmlu_pro_plus_biology": {
      "original": 717,
      "effective": 100
    },
    "mmlu_pro_plus_business": {
      "original": 789,
      "effective": 100
    },
    "mmlu_pro_plus_chemistry": {
      "original": 1132,
      "effective": 100
    },
    "mmlu_pro_plus_computer_science": {
      "original": 410,
      "effective": 100
    },
    "mmlu_pro_plus_economics": {
      "original": 844,
      "effective": 100
    },
    "mmlu_pro_plus_engineering": {
      "original": 969,
      "effective": 100
    },
    "mmlu_pro_plus_health": {
      "original": 818,
      "effective": 100
    },
    "mmlu_pro_plus_history": {
      "original": 381,
      "effective": 100
    },
    "mmlu_pro_plus_law": {
      "original": 1101,
      "effective": 100
    },
    "mmlu_pro_plus_math": {
      "original": 1351,
      "effective": 100
    },
    "mmlu_pro_plus_other": {
      "original": 924,
      "effective": 100
    },
    "mmlu_pro_plus_philosophy": {
      "original": 499,
      "effective": 100
    },
    "mmlu_pro_plus_physics": {
      "original": 1299,
      "effective": 100
    },
    "mmlu_pro_plus_psychology": {
      "original": 798,
      "effective": 100
    }
  },
  "config": {
    "model": "vllm",
    "model_args": "pretrained=/workspace/gemma3_12b_u2,dtype=auto,gpu_memory_utilization=0.9,max_model_len=4096",
    "batch_size": "auto:4",
    "batch_sizes": [],
    "device": "cuda:0",
    "use_cache": null,
    "limit": 100.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": null,
  "date": 1760451781.946239,
  "pretty_env_info": "PyTorch version: 2.7.1+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.8.93\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 5090\nNvidia driver version: 580.95.05\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.7.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.7.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        48 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               32\nOn-line CPU(s) list:                  0-31\nVendor ID:                            AuthenticAMD\nModel name:                           AMD Ryzen 9 7950X 16-Core Processor\nCPU family:                           25\nModel:                                97\nThread(s) per core:                   2\nCore(s) per socket:                   16\nSocket(s):                            1\nStepping:                             2\nCPU max MHz:                          5881.0000\nCPU min MHz:                          545.0000\nBogoMIPS:                             9000.03\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc amd_ibpb_ret arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                       AMD-V\nL1d cache:                            512 KiB (16 instances)\nL1i cache:                            512 KiB (16 instances)\nL2 cache:                             16 MiB (16 instances)\nL3 cache:                             64 MiB (2 instances)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-31\nVulnerability Gather data sampling:   Not affected\nVulnerability Itlb multihit:          Not affected\nVulnerability L1tf:                   Not affected\nVulnerability Mds:                    Not affected\nVulnerability Meltdown:               Not affected\nVulnerability Mmio stale data:        Not affected\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Not affected\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Not affected\nVulnerability Tsx async abort:        Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.2\n[pip3] nvidia-cublas-cu12==12.8.3.14\n[pip3] nvidia-cuda-cupti-cu12==12.8.57\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.61\n[pip3] nvidia-cuda-runtime-cu12==12.8.57\n[pip3] nvidia-cudnn-cu12==9.7.1.26\n[pip3] nvidia-cufft-cu12==11.3.3.41\n[pip3] nvidia-curand-cu12==10.3.9.55\n[pip3] nvidia-cusolver-cu12==11.7.2.55\n[pip3] nvidia-cusparse-cu12==12.5.7.53\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.8.61\n[pip3] nvidia-nvtx-cu12==12.8.55\n[pip3] optree==0.17.0\n[pip3] pytorch-triton==3.2.0+git4b3bb1f8\n[pip3] torch==2.7.1+cu128\n[pip3] torchao==0.14.0+cu128\n[pip3] torchaudio==2.7.1+cu128\n[pip3] torchvision==0.22.1+cu128\n[pip3] triton==3.3.1\n[conda] Could not collect",
  "transformers_version": "4.56.2",
  "lm_eval_version": "0.4.9.1",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<pad>",
    "0"
  ],
  "tokenizer_eos_token": [
    "<end_of_turn>",
    "106"
  ],
  "tokenizer_bos_token": [
    "<bos>",
    "2"
  ],
  "eot_token_id": 106,
  "max_length": 4096,
  "task_hashes": {
    "mmlu_pro_plus_biology": "6f6d2851363f322ebdb931c3725ffdbfbeaf43546c6f3d2e1e66ea8b56e1fa6f",
    "mmlu_pro_plus_business": "dec9f78278f46f5654f3521eebdf895f772f86f9a2744d56ee16d853fd650a61",
    "mmlu_pro_plus_chemistry": "6a6e3e3fb9f5c9d312b37c6044e73f28010e7ff8928b764832c0429f24e3656c",
    "mmlu_pro_plus_computer_science": "15bbe76533c262a4d0a670b177a7c64594752e9a2430a3ef7940dfcacf35a762",
    "mmlu_pro_plus_economics": "bb33fa06381438e978b70fc570435421dea8a44c5858bb1d9be6d59eb4153d18",
    "mmlu_pro_plus_engineering": "b0c3ccb5ab2f12fd9115c69422fb49bd510d3a8ae74b07ce118fe4602f6da310",
    "mmlu_pro_plus_health": "473895c8304ac985dedb2ff834bd670aa2a1dfae86b66a5406c65121ce9dd940",
    "mmlu_pro_plus_history": "c785550a0dcedd9bf9d561200a9f36fb873b86169935499ab5162889730909c7",
    "mmlu_pro_plus_law": "32d344e849c84fd86b931f01c327e945b3c40418a7ee7bfc8fe0e45b31a4c262",
    "mmlu_pro_plus_math": "661abab4d3c56ac3577caf3c1904118ea74373f0bcbe82cc644b792a5f75a765",
    "mmlu_pro_plus_other": "2ad845293f455bbdc170ee7507f2fcc1c33275837ece67be4ecc64c1aaed6bb0",
    "mmlu_pro_plus_philosophy": "000ef59ee6b63ae9d150b1e030beefdaf8b56869844a07758f8b470eee394410",
    "mmlu_pro_plus_physics": "41229cab3c9a2c0e7ef6dfafbddf1ec4ce37d177313680a8be9036f34080d104",
    "mmlu_pro_plus_psychology": "d3b49d65247cc9899397ae648d072fd254cf812263606ee142a780433fab2d43"
  },
  "model_source": "vllm",
  "model_name": "/workspace/gemma3_12b_u2",
  "model_name_sanitized": "__workspace__gemma3_12b_u2",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{ '<start_of_turn>model\n' }}\n{%- endif -%}\n",
  "chat_template_sha": "3cb7c5da795557bca16f7dfc2c8784d575870b51b1b00f8405896498fccdea3c",
  "start_time": 149737.492144378,
  "end_time": 151036.286743544,
  "total_evaluation_time_seconds": "1298.7945991660235"
}